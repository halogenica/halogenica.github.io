<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Graphics on Halogenica</title>
    <link>https://halogenica.github.io/graphics/</link>
    <description>Recent content in Graphics on Halogenica</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>mike@halogenica.net (Michael Romero)</managingEditor>
    <webMaster>mike@halogenica.net (Michael Romero)</webMaster>
    <lastBuildDate>Thu, 13 Mar 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://halogenica.github.io/graphics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sharing Resources Between DirectX and OpenGL</title>
      <link>https://halogenica.github.io/graphics/sharing-resources-between-directx-and-opengl/</link>
      <pubDate>Thu, 13 Mar 2014 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/sharing-resources-between-directx-and-opengl/</guid>
      <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://halogenica.github.io/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; &gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://halogenica.github.io/img/graphics/sharing-resources-between-directx-and-opengl/Shared_Resources.png&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://halogenica.github.io/img/graphics/sharing-resources-between-directx-and-opengl/Shared_Resources.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;I&amp;rsquo;ve recently had a need to simultaneously render using both DirectX and OpenGL. There are a number of reasons why this is useful, particularly in tools where you may wish to compare multiple rendering engines simultaneously. With this technique it is also possible to efficiently perform some rendering operations on one API to a render target, and switch to the other API to continue rendering to that render target. It can also be used to perform all rendering in a specific API, while presenting that final render target using another API. Providing direct access to textures and render targets in graphics memory regardless of API has the potential of efficiently pipelining surfaces through multiple discrete renderers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://halogenica.github.io/bin/graphics/sharing-resources-between-directx-and-opengl/WGL_NV_DX_interop_demo.zip&#34;&gt;WGL_NV_DX_interop_demo.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Signed Distance Fields – Part 1</title>
      <link>https://halogenica.github.io/graphics/signed-distance-fields/</link>
      <pubDate>Fri, 16 Nov 2012 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/signed-distance-fields/</guid>
      <description>&lt;p&gt;So there is this cool technique that had gained significant popularity in the demoscene called &amp;ldquo;Signed Distance Fields&amp;rdquo;. There&amp;rsquo;s a truly excellent presentation by iq of rgba (Iñigo Quilez) posted on his website &lt;a href=&#34;http://www.iquilezles.org/www/material/nvscene2008/nvscene2008.htm&#34;&gt;http://www.iquilezles.org&lt;/a&gt; which he presented at nvscene back in 2008 called &amp;ldquo;Rendering Worlds with Two Triangles&amp;rdquo;. I wanted to play around with some GLSL and thought this would be a really interesting algorithm to take a look at. You can see some of the power of these types of functions in a presentation that smash of fairlight (Matt Swaboda) gave at GDC earlier this year &lt;a href=&#34;http://directtovideo.wordpress.com/2012/03/15/get-my-slides-from-gdc2012/&#34;&gt;http://directtovideo.wordpress.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Particle System</title>
      <link>https://halogenica.github.io/graphics/particle-system/</link>
      <pubDate>Wed, 22 Dec 2010 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/particle-system/</guid>
      <description>&lt;p&gt;I saw a video online recently of several cubes flying into the scene and stacking up to form a larger cube. I thought it was a pretty simple yet powerful effect. The effect was achieved using Particle Flow, so the movement of the particles are entirely animated and not in real time. After seeing this, I immediately wanted to try it, but with a more general solution. So, off I went to write a physics system for my particle engine.&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/PXYOYEgjDXQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Volume Rendering using GPGPU</title>
      <link>https://halogenica.github.io/graphics/volume-rendering-using-gpgpu/</link>
      <pubDate>Mon, 15 Jun 2009 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/volume-rendering-using-gpgpu/</guid>
      <description>&lt;p&gt;Traditional 3D computer graphics focus on rendering the exterior of objects. Volume rendering is a technique used to visualize information corresponding to the interior of an object, commonly used in medical imaging and other fields. Visualization of such data may be accomplished by ray casting; an embarrassingly parallel algorithm also commonly used in ray tracing. There has been growing interest in performing general purpose computations on graphics processing units (GPGPU), which are capable exploiting parallel applications and yielding far greater performance than sequential implementations on CPUs. Modern GPUs allow for rapid acceleration of volume rendering applications, offering affordable high performance visualization systems.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Volume Rendering on the PSP</title>
      <link>https://halogenica.github.io/graphics/volume-rendering-on-the-psp/</link>
      <pubDate>Sun, 15 Feb 2009 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/volume-rendering-on-the-psp/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve long been active in PSP development and decided to have some fun with graphics on the PSP. I asked my university to order two PSP dev systems and I got to work porting my ray tracer to the PSP. As a team effort with three other graphics enthusiasts, we decided to accelerate hypertexturing on the PSP using the VFPU. We ended up with a ray tracer, hypertexturer, and volume ray caster working on the PSP capable of displaying a 64x64x64 voxel volume.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hypertexturing</title>
      <link>https://halogenica.github.io/graphics/hypertexturing/</link>
      <pubDate>Mon, 19 Jan 2009 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/hypertexturing/</guid>
      <description>&lt;p&gt;This code is branched from the Ray Tracer code, with the perlin noise function added and the hit function and normal calculations for the volume of noise/density.&lt;/p&gt;

&lt;video loop autoplay src=&#34;https://halogenica.github.io/img/graphics/hypertexturing/reflective_hypertexture.mp4&#34;&gt;&lt;/video&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ray Tracer</title>
      <link>https://halogenica.github.io/graphics/ray-tracer/</link>
      <pubDate>Mon, 15 Dec 2008 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/ray-tracer/</guid>
      <description>&lt;p&gt;This was my first ray tracer, implemented on the CPU using OpenGL and &lt;a href=&#34;http://www.opengl.org/resources/libraries/glut/&#34;&gt;GLUT&lt;/a&gt; to present to the screen, and coded in &lt;a href=&#34;http://www.bloodshed.net/devcpp.html&#34;&gt;Dev-C++&lt;/a&gt;. It is largely inspired by the ray tracer used in the book &lt;a href=&#34;http://www.raytracegroundup.com/&#34;&gt;Ray Tracing from the Ground Up&lt;/a&gt;, an excellent and highly recommended book on ray tracing by Kevin Suffern.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>First Graphics Project</title>
      <link>https://halogenica.github.io/graphics/first-graphics-project/</link>
      <pubDate>Fri, 14 Dec 2007 00:00:00 +0000</pubDate>
      <author>mike@halogenica.net (Michael Romero)</author>
      <guid>https://halogenica.github.io/graphics/first-graphics-project/</guid>
      <description>&lt;p&gt;My very first graphics project. It&amp;rsquo;s written in C, and uses old fixed-function &lt;a href=&#34;http://www.opengl.org/&#34;&gt;OpenGL&lt;/a&gt; and &lt;a href=&#34;http://www.opengl.org/resources/libraries/glut/&#34;&gt;GLUT&lt;/a&gt;. The code is pretty sloppy as most first attempts go. There are a few interactive elements to this project, with the box spreading apart and interesting use of blending. As the transparent surfaces occlude other geometry, the geometry &amp;ldquo;disappears&amp;rdquo; and shapes are revealed inside the formerly opaque geometry.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>